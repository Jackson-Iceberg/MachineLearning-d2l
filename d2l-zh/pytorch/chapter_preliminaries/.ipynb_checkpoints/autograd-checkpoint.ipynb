{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d54ece0",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自动微分\n",
    ":label:`sec_autograd`\n",
    "\n",
    "正如我们在 :numref:`sec_calculus`中所说的那样，求导是几乎所有深度学习优化算法的关键步骤。\n",
    "虽然求导的计算很简单，只需要一些基本的微积分。\n",
    "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
    "\n",
    "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
    "实际中，根据我们设计的模型，系统会构建一个*计算图*（computational graph），\n",
    "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "自动微分使系统能够随后反向传播梯度。\n",
    "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "## 一个简单的例子\n",
    "\n",
    "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f462b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:05.865562Z",
     "iopub.status.busy": "2022-07-31T02:37:05.864988Z",
     "iopub.status.idle": "2022-07-31T02:37:06.545457Z",
     "shell.execute_reply": "2022-07-31T02:37:06.544791Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985f146",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度。**]\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a89d68f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.548917Z",
     "iopub.status.busy": "2022-07-31T02:37:06.548534Z",
     "iopub.status.idle": "2022-07-31T02:37:06.552043Z",
     "shell.execute_reply": "2022-07-31T02:37:06.551427Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11394bbe",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "(**现在让我们计算$y$。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275face9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.554961Z",
     "iopub.status.busy": "2022-07-31T02:37:06.554622Z",
     "iopub.status.idle": "2022-07-31T02:37:06.560002Z",
     "shell.execute_reply": "2022-07-31T02:37:06.559374Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c39a2",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
    "接下来，我们[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a562bf7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.562926Z",
     "iopub.status.busy": "2022-07-31T02:37:06.562592Z",
     "iopub.status.idle": "2022-07-31T02:37:06.629473Z",
     "shell.execute_reply": "2022-07-31T02:37:06.628828Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6e3b3",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02955afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.632562Z",
     "iopub.status.busy": "2022-07-31T02:37:06.632210Z",
     "iopub.status.idle": "2022-07-31T02:37:06.637390Z",
     "shell.execute_reply": "2022-07-31T02:37:06.636762Z"
    },
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e2c2c",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "[**现在让我们计算`x`的另一个函数。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d26fa2e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.640282Z",
     "iopub.status.busy": "2022-07-31T02:37:06.639960Z",
     "iopub.status.idle": "2022-07-31T02:37:06.645418Z",
     "shell.execute_reply": "2022-07-31T02:37:06.644814Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43640fcd",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## 非标量变量的反向传播\n",
    "\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
    "但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8702b8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.648225Z",
     "iopub.status.busy": "2022-07-31T02:37:06.647897Z",
     "iopub.status.idle": "2022-07-31T02:37:06.653894Z",
     "shell.execute_reply": "2022-07-31T02:37:06.653269Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665d5bb",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## 分离计算\n",
    "\n",
    "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，我们希望将`y`视为一个常数，\n",
    "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
    "\n",
    "在这里，我们可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
    "但丢弃计算图中如何计算`y`的任何信息。\n",
    "换句话说，梯度不会向后流经`u`到`x`。\n",
    "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
    "而不是`z=x*x*x`关于`x`的偏导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef1336d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.656825Z",
     "iopub.status.busy": "2022-07-31T02:37:06.656481Z",
     "iopub.status.idle": "2022-07-31T02:37:06.662411Z",
     "shell.execute_reply": "2022-07-31T02:37:06.661807Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482b7e7",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
    "得到`y=x*x`关于的`x`的导数，即`2*x`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25d6074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.665549Z",
     "iopub.status.busy": "2022-07-31T02:37:06.665027Z",
     "iopub.status.idle": "2022-07-31T02:37:06.670720Z",
     "shell.execute_reply": "2022-07-31T02:37:06.670092Z"
    },
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66954a77",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Python控制流的梯度计算\n",
    "\n",
    "使用自动微分的一个好处是：\n",
    "[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。\n",
    "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3a4e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.673776Z",
     "iopub.status.busy": "2022-07-31T02:37:06.673308Z",
     "iopub.status.idle": "2022-07-31T02:37:06.677303Z",
     "shell.execute_reply": "2022-07-31T02:37:06.676673Z"
    },
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae128b06",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "让我们计算梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25d3408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.680229Z",
     "iopub.status.busy": "2022-07-31T02:37:06.679765Z",
     "iopub.status.idle": "2022-07-31T02:37:06.684566Z",
     "shell.execute_reply": "2022-07-31T02:37:06.683900Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb899a",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "我们现在可以分析上面定义的`f`函数。\n",
    "请注意，它在其输入`a`中是分段线性的。\n",
    "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`。\n",
    "因此，我们可以用`d/a`验证梯度是否正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df0be3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.687918Z",
     "iopub.status.busy": "2022-07-31T02:37:06.687447Z",
     "iopub.status.idle": "2022-07-31T02:37:06.692256Z",
     "shell.execute_reply": "2022-07-31T02:37:06.691639Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f59b1",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
    "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果我们将变量`a`更改为随机向量或矩阵，会发生什么？\n",
    "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eefc779",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '，' (U+FF0C) (789445712.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [13]\u001b[1;36m\u001b[0m\n\u001b[1;33m    1. 计算二阶导数是在一阶导数的基础上进行的，自然开销要大。\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '，' (U+FF0C)\n"
     ]
    }
   ],
   "source": [
    "# 1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
    "## Answer\n",
    "1. 计算二阶导数是在一阶导数的基础上进行的，自然开销要大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1e7f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   4.,   8.,  12.,  16.,  20.,  24.,  28.,  32.,  36.,  40.,  44.,\n",
       "         48.,  52.,  56.,  60.,  64.,  68.,  72.,  76.,  80.,  84.,  88.,  92.,\n",
       "         96., 100., 104., 108., 112., 116., 120., 124., 128., 132., 136., 140.,\n",
       "        144., 148., 152., 156.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "import torch\n",
    "x = torch.arange(40.,requires_grad=True)\n",
    "y = 2 * torch.dot(x**2,torch.ones_like(x))\n",
    "y.backward()\n",
    "x.grad\n",
    "# y.backward() \n",
    "# RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9d5ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[ 0.1683],\n",
      "        [-0.4781],\n",
      "        [-0.3838]], requires_grad=True)\n",
      "\n",
      " tensor(1.2716, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(2.5431, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(5.0863, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(10.1726, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(20.3452, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(40.6904, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(81.3807, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(162.7615, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(325.5230, grad_fn=<CopyBackwards>)\n",
      "\n",
      " tensor(651.0460, grad_fn=<CopyBackwards>)\n",
      "c=100b\n",
      " tensor([[ 34467.1328],\n",
      "        [-97920.3281],\n",
      "        [-78600.6406]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[0;32m     18\u001b[0m d \u001b[38;5;241m=\u001b[39m f(a)\n\u001b[1;32m---> 19\u001b[0m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#<====== run time error if a is vector or matrix RuntimeError: grad can be implicitly created only for scalar outputs\u001b[39;00m\n\u001b[0;32m     20\u001b[0m d\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m#<===== this way it will work\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    165\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 166\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# 在控制流的例子中，我们计算d关于a的导数，如果我们将变量a更改为随机向量或矩阵，会发生什么？\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        print(\"\\n\", b.norm())\n",
    "        b = b * 2\n",
    "        if b.sum() > 0:\n",
    "            c = b\n",
    "            print(\"C==b\\n\", c)\n",
    "    else:\n",
    "        c = 100 * b\n",
    "        print(\"c=100b\\n\", c)\n",
    "        return c\n",
    "\n",
    "a = torch.randn(size=(3,1), requires_grad=True)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "d = f(a)\n",
    "d.backward() #<====== run time error if a is vector or matrix RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "d.sum().backward() #<===== this way it will work\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e2badd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     c\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m-\u001b[39mb\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c\n\u001b[1;32m----> 5\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def f(a):\n",
    "    b=a**2+abs(a)\n",
    "    c=b**3-b**(-4)\n",
    "    return c\n",
    "a = torch.randn(size=(3,1), requires_grad=True)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "d = f(a)\n",
    "d.sum().backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from matplotlib.ticker import FuncFormatter, MultipleLocator\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "f,ax=plt.subplots(1)\n",
    "\n",
    "x = np.linspace(-3*np.pi, 3*np.pi, 100)\n",
    "x1= torch.tensor(x, requires_grad=True)\n",
    "y1= torch.sin(x1)\n",
    "y1.sum().backward()\n",
    "\n",
    "ax.plot(x,np.sin(x),label=\"sin(x)\")\n",
    "ax.plot(x,x1.grad,label=\"gradient of sin(x)\")\n",
    "ax.legend(loc=\"upper center\", shadow=True)\n",
    "\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(\n",
    "lambda val,pos: \"{:.0g}\\pi\".format(val/np.pi) if val !=0 else \"0\"\n",
    "))\n",
    "ax.xaxis.set_major_locator(MultipleLocator(base=np.pi))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a99037",
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1759)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
